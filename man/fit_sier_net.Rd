% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_sier_net.R
\name{fit_sier_net}
\alias{fit_sier_net}
\title{Fits a sparse-input hierarchical network}
\usage{
fit_sier_net(
  x_train,
  y_train,
  loss,
  loss_func,
  num_layers,
  num_hidden_nodes,
  num_out,
  lambda1,
  lambda2,
  epochs_adam,
  epochs_prox,
  validation_split = 0,
  batch_size = 100,
  sample_weight = NULL,
  learning_rate = 1e-04,
  seed = 1
)
}
\arguments{
\item{x_train}{matrix with rows as observations, columns as covariates}

\item{y_train}{the observed responses for each observation, should be the number of the class (0-indexed)}

\item{loss}{the name of the loss function to use ("mse" or "sparse_categorical_crossentropy")}

\item{loss_func}{the loss function to use (keras::loss_mean_squared_error or keras::loss_sparse_categorical_crossentropy)}

\item{num_layers}{number of hidden layer}

\item{num_hidden_nodes}{number of hidden nodes per layer}

\item{num_out}{number of outputs}

\item{lambda1}{penalty parameter for input sparsity (lambda1 in the paper)}

\item{lambda2}{penalty parameter for network size (lambda2 in the paper)}

\item{epochs_adam}{number of epochs to run Adam}

\item{epochs_prox}{number of epochs to run proximal gradient descent}

\item{validation_split}{proportion of data to split for validation}

\item{batch_size}{the size of the mini-batch in Adam}

\item{sample_weight}{how much to weight each observation, optional}

\item{learning_rate}{the step size/learning rate for the optimization algorithms}

\item{seed}{random seed for initializing weights}
}
\value{
A fitted sparse-input hierarchical network (a pytorch object)
}
\description{
Fits a sparse-input hierarchical network
}
